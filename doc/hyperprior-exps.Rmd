---
title: "experiments with hyperprior"
author: "asif zubair"
date: "8/20/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r utility, echo=FALSE}
library(deconR)
makeStanData <- function(noisefunc, ...){
  # Create a simulated cancer datasets.
  # 600 genes, 1000 patients, genes expression goes from 1:10 in even steps with gaussian noise.
  cancerExpressionMat <- numeric(1000*600)
  dim(cancerExpressionMat) <- c(600, 1000)
  normalExpressionMat  <- numeric(1000*600)
  dim(normalExpressionMat) <- c(600, 1000)
  p_bulkExpressionSimMat <-numeric(1000*600)
  dim(p_bulkExpressionSimMat) <- c(600, 1000)

  for(i in 1:1000)
  {
    cancerExpressionMat[,i] <- seq(1, 1.599, .001) + noisefunc(...)
    normalExpressionMat[,i] <- seq(1.599, 1, -.001) + noisefunc(...)
    p_bulkExpressionSimMat[, i] <- (cancerExpressionMat[,i] * p_theProp[i]) + (normalExpressionMat[,i] * p_propInv[i])
  } 
  
  # the values before noise was added
  p_cancerSig <- seq(1, 1.599, .001)
  # QUESTION: why do we take the mean here ?
  # Did you mean to do this:
  # p_normalSig <- seq(1.599, 1, -.001) ?
  p_normalSig <- apply(normalExpressionMat, 1, mean)
  p_simSigMatTwo <- cbind(p_cancerSig, p_normalSig)
  
  numGenes = nrow(p_bulkExpressionSimMat)
  numCellTypes = ncol(p_simSigMatTwo)
  standata <- list(numGenes = numGenes, numCellTypes = numCellTypes, 
                   exprMixVec = p_bulkExpressionSimMat[,1], sigMat = p_simSigMatTwo)
  return(standata)
}
```


## Motivation

We want to investigate the role/performance of the hyperprior when used in the deconvolution algorithm. We will use some simple simulations to do this. 

## Normal distribution

Let's say that the error is normally distributed. Thus:

```{r normal}
standata <- makeStanData(noisefunc = rnorm, n = 600, mean = 0, sd = .2)  
```

The output from the sampling algorithm with and without hyperprior is below:

```{r norm.out}
rstan::sampling(deconR:::stanmodels$indSigmat, data = standata, 
                verbose = F, refresh = 0);
rstan::sampling(deconR:::stanmodels$indSigmatHyperprior, data = standata, 
                verbose = F, refresh = 0)
```

We see that the `nu` value is close to 49.33. This is becasue of the the normal error model that we use in the simulation. Essentially, with `df` greater than 30, the t-distribution behaves like a normal. However, we need to still be circumspect about how much influence the prior has on the `nu` value. 

Let's do a few more tests to see if prior influence is indeed driving this. 

## t distribution with df = 4

```{r t4}
standata <- makeStanData(noisefunc = deconR:::rgt, 
                         n = 600, df = 4, mu = 0, sigma = 1.2)
```

Let's look at the output now:

```{r t4.out}
rstan::sampling(deconR:::stanmodels$indSigmat, data = standata, verbose = F, refresh = 0);
rstan::sampling(deconR:::stanmodels$indSigmatHyperprior, data = standata, verbose = F, refresh = 0)
```

We see that the hyperprior model recovers that `nu = 5.45` which is pretty close to the actual value of 4. 

## t distribution with df = 200

```{r t200}
standata <- makeStanData(noisefunc = deconR:::rgt, 
                         n = 600, df = 200, mu = 0, sigma = 1.2)
```

Again, computing the output:

```{r t200.out}
rstan::sampling(deconR:::stanmodels$indSigmat, data = standata, 
                verbose = F, refresh = 0);
rstan::sampling(deconR:::stanmodels$indSigmatHyperprior, data = standata, 
                verbose = F, refresh = 0)
```

Now we see tha the hyperprior model recovers that `nu = 25.56` which is way off of the 200 true value. 

## Conclusion

It seems that the prior model for `nu` that we use (`gamma(2, 0.1)`) happens to be informative. This is why it struggles to recover really high values of the degree of freedom. It is possible that the prior we were using previously, `gamma(2, 0.01)`, was weakly informative and thus we were able to recover both high and low values for the degrees of freedom. 


